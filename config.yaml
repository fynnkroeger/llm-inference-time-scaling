llm_params:
  model: meta-llama/Llama-3.2-1B
  gpu_memory_utilization: 0.75
sampling:
  temperature: 0.8
  top_p: 0.95
  max_tokens: 2
  n: 4 # number of samples
evaluate: true
