llm_params:
  model: meta-llama/Llama-3.2-1B
  gpu_memory_utilization: 0.75
sampling:
  temperature: 0.8
  top_p: 0.95
  max_tokens: 2
  num_samples: 4
evaluate: true
